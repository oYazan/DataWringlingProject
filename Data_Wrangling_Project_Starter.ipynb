{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNIwe5N7s0e_"
   },
   "source": [
    "# Real-world Data Wrangling"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Better visualizations for Pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', 1000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDYDkH-Zs7Nn"
   },
   "source": "## 1. Gather data"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbN7z7rcuqpO"
   },
   "source": [
    "### **1.1.** Problem Statement\n",
    "This project aims to explore the relationship between mobility patterns and COVID-19 transmission during the pandemic,\n",
    "by combining COVID-19 cases and Google mobility trends, I am going to discuss how changes in human\n",
    "mobility with infection rates and identify which factors of mobility were most highly correlated\n",
    "with COVID-19 being spread in different areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AQfBAdUypMm"
   },
   "source": "### **1.2.** Gather at least two datasets using two different data gathering methods"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e6gS0wL1KTu"
   },
   "source": [
    "#### **COVID-19 Data**\n",
    "\n",
    "Type: *CSV File*\n",
    "\n",
    "Method: *Programmatically downloading files from GitHub repository*\n",
    "\n",
    "Dataset variables:\n",
    "\n",
    "* *Province/State*: Province, state or dependency name\n",
    "* *Country/Region*: Country, region or sovereignty name\n",
    "* *Lat*: Latitude coordinate\n",
    "* *Long*: Longitude coordinate\n",
    "* *Multiple date columns*: Each column represents confirmed COVID-19 cases on that date"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Su8E0uLuYkHU"
   },
   "source": [
    "# Define the COVID-19 global confirmed cases link\n",
    "url_confirmed = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"\n",
    "\n",
    "# Download the data\n",
    "response_confirmed = requests.get(url_confirmed)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response_confirmed.status_code == 200:\n",
    "    # Convert to DataFrame\n",
    "    covid_data = pd.read_csv(io.StringIO(response_confirmed.text))\n",
    "\n",
    "    # Create a directory for storing data if it doesn't exist\n",
    "    os.makedirs('data/raw', exist_ok=True)\n",
    "\n",
    "    # Save the confirmed cases CSV file\n",
    "    covid_data.to_csv('data/raw/confirmed_cases.csv', index=False)\n",
    "\n",
    "    # Display first few rows of the dataset\n",
    "    print(covid_data.head())\n",
    "else:\n",
    "    print(f\"Failed to download confirmed cases data. Status code: {response_confirmed.status_code}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoUjq1tPzz7P"
   },
   "source": [
    "#### **Google Mobility Data**\n",
    "\n",
    "Type: *CSV File*\n",
    "\n",
    "Method: *Web scraping using BeautifulSoup to find and download from Google's COVID-19 Mobility Reports website*\n",
    "\n",
    "Dataset variables:\n",
    "\n",
    "* *country_region_code*: Two-letter country code\n",
    "* *country_region*: Name of the country\n",
    "* *sub_region_1*: Name of the state/province/region\n",
    "* *sub_region_2*: Name of the county/municipality\n",
    "* *metro_area*: Name of the metropolitan area\n",
    "* *iso_3166_2_code*: ISO 3166-2 code for the region\n",
    "* *census_fips_code*: FIPS code for the county\n",
    "* *date*: Date in YYYY-MM-DD format\n",
    "* *retail_and_recreation_percent_change_from_baseline*: Mobility trends for places like restaurants, cafes, shopping centers\n",
    "* *grocery_and_pharmacy_percent_change_from_baseline*: Mobility trends for grocery markets, food warehouses, pharmacies\n",
    "* *parks_percent_change_from_baseline*: Mobility trends for parks, beaches, public gardens\n",
    "* *transit_stations_percent_change_from_baseline*: Mobility trends for public transport hubs\n",
    "* *workplaces_percent_change_from_baseline*: Mobility trends for places of work\n",
    "* *residential_percent_change_from_baseline*: Mobility trends for places of residence"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6zT0QxRyYmm7"
   },
   "source": [
    "# URL for Google's COVID-19 Mobility Reports website\n",
    "google_mobility_url = \"https://www.google.com/covid19/mobility/\"\n",
    "\n",
    "# Send a request to the website\n",
    "response = requests.get(google_mobility_url)\n",
    "\n",
    "# Check the request\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the link to the global CSV file\n",
    "    # The global CSV file link typically contains \"Global_Mobility_Report.csv\"\n",
    "    csv_links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if 'Global_Mobility_Report.csv' in link['href']:\n",
    "            csv_links.append(link['href'])\n",
    "\n",
    "    if csv_links:\n",
    "        # Take the first matching link\n",
    "        global_csv_link = csv_links[0]\n",
    "\n",
    "        # Download the CSV file\n",
    "        csv_response = requests.get(global_csv_link)\n",
    "\n",
    "        if csv_response.status_code == 200:\n",
    "            # Create a directory for storing raw data if it doesn't exist\n",
    "            os.makedirs('data/raw', exist_ok=True)\n",
    "\n",
    "            # Save the raw CSV file\n",
    "            with open('data/raw/Global_Mobility_Report.csv', 'wb') as f:\n",
    "                f.write(csv_response.content)\n",
    "\n",
    "            # Load a sample of the data\n",
    "            mobility_data = pd.read_csv('data/raw/Global_Mobility_Report.csv')\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to download CSV file. Status code: {csv_response.status_code}\")\n",
    "    else:\n",
    "        # If we can't find the global CSV link, look for country-specific CSV files\n",
    "        country_links = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            if re.search(r'Mobility_Report_[A-Z]{2}\\.csv', link['href']):\n",
    "                country_links.append(link['href'])\n",
    "\n",
    "        if country_links:\n",
    "            # Download a few country-specific files as samples\n",
    "            print(f\"Found {len(country_links)} country-specific mobility report links\")\n",
    "            selected_countries = []\n",
    "            for link in country_links:\n",
    "                country_code = re.search(r'Mobility_Report_([A-Z]{2})\\.csv', link).group(1)\n",
    "                if country_code in ['US', 'GB', 'DE', 'FR', 'IT']:\n",
    "                    selected_countries.append((country_code, link))\n",
    "\n",
    "            # Create directory for raw data\n",
    "            os.makedirs('data/raw', exist_ok=True)\n",
    "\n",
    "            # Download selected country files\n",
    "            for country_code, link in selected_countries:\n",
    "                country_response = requests.get(link)\n",
    "                if country_response.status_code == 200:\n",
    "                    with open(f'data/raw/Mobility_Report_{country_code}.csv', 'wb') as f:\n",
    "                        f.write(country_response.content)\n",
    "                    print(f\"Saved mobility data for {country_code}\")\n",
    "\n",
    "            # Load one of the country files as a sample\n",
    "            if selected_countries:\n",
    "                sample_country = selected_countries[0][0]\n",
    "                mobility_data = pd.read_csv(f'data/raw/Mobility_Report_{sample_country}.csv')\n",
    "                print(f\"Loaded mobility data for {sample_country} as a sample\")\n",
    "        else:\n",
    "            print(\"Could not find any mobility report CSV links on the page.\")\n",
    "else:\n",
    "    print(f\"Failed to access the website. Status code: {response.status_code}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwSWIVmotLgV"
   },
   "source": [
    "## 2. Assess data\n",
    "\n",
    "Assess the data according to data quality and tidiness metrics using the report below.\n",
    "\n",
    "List **two** data quality issues and **two** tidiness issues. Assess each data issue visually **and** programmatically, then briefly describe the issue you find.  **Make sure you include justifications for the methods you use for the assessment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adaK2iPNzVu4"
   },
   "source": "### Quality Issue 1: Missing values in COVID-19 dataset"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SpW59kh-zl8d"
   },
   "source": [
    "# Inspecting the dataframe visually\n",
    "\n",
    "print(\"\\nCOVID-19 Dataset Info:\")\n",
    "covid_data.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-qfcocStzsKg"
   },
   "source": [
    "# Inspecting the dataframe programmatically\n",
    "missing_values_covid = covid_data.isnull().sum()\n",
    "print(\"\\nMissing values in COVID-19 dataset:\")\n",
    "print(missing_values_covid[missing_values_covid > 0])\n",
    "\n",
    "# Visualize missing data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(covid_data.isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
    "plt.title('Missing Values in COVID-19 Dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Issue and justification: *The COVID-19 data contains missing values in the 'Province/State' column. This is a quality issue because missing data will affect our analysis, especially if we are interested in analyzing trends at the provincial/state level. I used the.info() function to get a summary of the data types and missing values, then counted the number of missing values in each column explicitly. The visual heatmap confirms where the missingness in our data is.*"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Be77N4I1AmE"
   },
   "source": "### Quality Issue 2: Inconsistent date formats in Mobility dataset"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iMhHyiyLM2I3"
   },
   "source": [
    "# Inspecting the dataframe visually\n",
    "\n",
    "print(\"\\nMobility Dataset Date Column:\")\n",
    "print(mobility_data['date'].head(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bnviRCUI-bb7"
   },
   "source": [
    "# Inspecting the dataframe programmatically\n",
    "\n",
    "print(\"\\nDate column data type:\", mobility_data['date'].dtype)\n",
    "date_sample = mobility_data['date'].sample(10)\n",
    "print(\"\\nSample of dates:\")\n",
    "print(date_sample)\n",
    "# Check if dates are in consistent format\n",
    "try:\n",
    "    # Convert to datetime\n",
    "    pd.to_datetime(mobility_data['date'])\n",
    "    print(\"All dates are in a consistent format that pandas can parse\")\n",
    "except Exception as e:\n",
    "    print(f\"Inconsistent date formats detected: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Issue and justification: *Even though not obvious within our given sample, most real-world data will contain inhomogeneous date formats. Within this case, the 'date' column within the mobility dataframe is stored as a string object rather than as an actual datetime type. This is a quality issue because it can lead to incorrect time-based analyses and makes time-series operations cumbersome. I confirmed this by looking at the data type of the date column and inspecting a sample of date values to ensure the storage format.*"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXhGiYyiwwKN"
   },
   "source": "### Tidiness Issue 1: COVID-19 data in wide format (dates as columns)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fleC5rORI0Xl"
   },
   "source": [
    "# Inspecting the dataframe visually\n",
    "\n",
    "print(\"\\nCOVID-19 Dataset Columns (sample):\")\n",
    "print(covid_data.columns[:10])  # Show first 10 columns\n",
    "print(\"...\")\n",
    "print(covid_data.columns[-5:])  # Show last 5 columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BTuQw7Rbsio4"
   },
   "source": [
    "#Inspecting the dataframe programmatically\n",
    "\n",
    "date_columns = [col for col in covid_data.columns if '/' in col]\n",
    "print(f\"\\nNumber of date columns: {len(date_columns)}\")\n",
    "print(f\"First few date columns: {date_columns[:5]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Issue and justification: *The COVID-19 dataset is in a wide format with column names as dates, which violates the tidy data rule of having each variable be a column. This format complicates time-series analysis and joining with other datasets as well. I identified this issue by checking the column names and ensuring most columns are dates and not variables. This untidy format would need to be reshaped to long format in order to be analyzed correctly.*"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ffMoRGSwzYj"
   },
   "source": "### Tidiness Issue 2: Multiple location hierarchies in separate columns in both datasets"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XUpeoqokw5Qt"
   },
   "source": [
    "#Inspecting the dataframe visually\n",
    "print(\"\\nLocation columns in COVID-19 dataset:\")\n",
    "print(covid_data[['Province/State', 'Country/Region']].head())\n",
    "\n",
    "print(\"\\nLocation columns in Mobility dataset:\")\n",
    "location_cols = ['country_region', 'sub_region_1', 'sub_region_2', 'metro_area']\n",
    "print(mobility_data[location_cols].head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c8JK4DoXxtFA"
   },
   "source": [
    "# Inspecting the dataframe programmatically\n",
    "# Check uniqueness of location combinations\n",
    "covid_locations = covid_data.groupby(['Country/Region']).size().reset_index(name='count')\n",
    "print(f\"\\nNumber of unique countries in COVID-19 data: {len(covid_locations)}\")\n",
    "\n",
    "mobility_locations = mobility_data.groupby(['country_region', 'sub_region_1']).size().reset_index(name='count')\n",
    "print(f\"Number of country-region combinations in Mobility data: {len(mobility_locations)}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Issue and justification: *Issue and rationale: Both datasets contain more than one column for geographic hierarchy (country, state/province, etc.) with different naming conventions and granularity levels. This is a tidiness issue because it complicates joining the datasets and makes them redundant. I inspected the location-related columns in both datasets and counted distinct combinations to identify the granularity of each dataset. To enable effective analysis, these location hierarchies need to be standardized and potentially aggregated.*"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6gmLnBttpCh"
   },
   "source": [
    "## 3. Clean data\n",
    "Clean the data to solve the 4 issues corresponding to data quality and tidiness found in the assessing step. **Make sure you include justifications for your cleaning decisions.**\n",
    "\n",
    "After the cleaning for each issue, please use **either** the visually or programatical method to validate the cleaning was succesful.\n",
    "\n",
    "At this stage, you are also expected to remove variables that are unnecessary for your analysis and combine your datasets. Depending on your datasets, you may choose to perform variable combination and elimination before or after the cleaning stage. Your dataset must have **at least** 4 variables after combining the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Make copies of the datasets to ensure the raw dataframes are not impacted\n",
    "covid_clean = covid_data.copy()\n",
    "mobility_clean = mobility_data.copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmFhN52Yyn3l"
   },
   "source": "### **Quality Issue 1: Missing values in COVID-19 dataset**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9UejDWrNMW4a"
   },
   "source": [
    "# Apply the cleaning strategy\n",
    "# For Province/State, we'll fill missing values with a placeholder for Country-level data\n",
    "covid_clean['Province/State'].fillna('Country-level', inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oUBee-LPytkv"
   },
   "source": [
    "# Validate the cleaning was successful\n",
    "missing_after = covid_clean.isnull().sum()\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(missing_after[missing_after > 0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Justification: *I filled the missing values in the 'Province/State' column with 'Country-level' to indicate that these records are country-level aggregates and not specific provinces or states. This preserves the hierarchical nature of the data and eliminates missing values. The validation indicates there are no missing values in the dataset now.*"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_DAUbJrymBL"
   },
   "source": "### **Quality Issue 2: Inconsistent date formats in Mobility dataset**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5Yfb-Yu5MTuE"
   },
   "source": [
    "# Apply the cleaning strategy\n",
    "\n",
    "# Convert date column to datetime format\n",
    "mobility_clean['date'] = pd.to_datetime(mobility_clean['date'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ionB2sRaMUmY"
   },
   "source": [
    "# Validate the cleaning was successful\n",
    "print(\"\\nDate column data type after cleaning:\", mobility_clean['date'].dtype)\n",
    "print(\"Sample dates after conversion:\")\n",
    "print(mobility_clean['date'].head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Justification: *I also converted the 'date' column from string type to proper datetime objects using pd.to_datetime(). The date format is normalized and enables proper time-series analysis, date range filtering, and resampling. The validation shows that the column now has type datetime64, proving a successful transformation.*"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIUrrfSNyOPR"
   },
   "source": "### **Tidiness Issue 1: COVID-19 data in wide format (dates as columns)**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fib0zAm333bn"
   },
   "source": [
    "# Apply the cleaning strategy\n",
    "\n",
    "# Reshape from wide to long format\n",
    "id_cols = ['Province/State', 'Country/Region', 'Lat', 'Long']\n",
    "date_cols = [col for col in covid_clean.columns if col not in id_cols]\n",
    "\n",
    "# Melt the dataframe to convert from wide to long format\n",
    "covid_long = pd.melt(\n",
    "    covid_clean,\n",
    "    id_vars=id_cols,\n",
    "    value_vars=date_cols,\n",
    "    var_name='date',\n",
    "    value_name='confirmed_cases'\n",
    ")\n",
    "\n",
    "# Convert the date column to datetime format\n",
    "covid_long['date'] = pd.to_datetime(covid_long['date'])\n",
    "\n",
    "# Sort the data\n",
    "covid_long.sort_values(['Country/Region', 'Province/State', 'date'], inplace=True)\n",
    "\n",
    "# Reset index\n",
    "covid_long.reset_index(drop=True, inplace=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yhrnUGY_Nk8B"
   },
   "source": [
    "# Validate the cleaning was successful\n",
    "\n",
    "print(\"\\nCOVID-19 data after reshaping to long format:\")\n",
    "print(covid_long.head())\n",
    "print(f\"Shape after reshaping: {covid_long.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Justification: *I reshaped the COVID-19 data from wide (dates as columns) to long format using pandas' melt function. This results in a tidy data frame with each row an individual observation (a specific location at a specific date) and each column a variable. I also converted the date strings to proper datetime objects and sorted the data for ease of analysis. The validation assures us that we now have a properly structured time-series data.*"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o51Bt8kwyTzk"
   },
   "source": "### **Tidiness Issue 2: Multiple location hierarchies in separate columns**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7zW8O5yx4Y9O"
   },
   "source": [
    "#Apply the cleaning strategy\n",
    "\n",
    "# 1. Standardize location column names in both datasets\n",
    "# 2. Create consistent location identifiers for joining\n",
    "# For COVID-19 data:\n",
    "covid_long = covid_long.rename(columns={\n",
    "    'Country/Region': 'country',\n",
    "    'Province/State': 'region'\n",
    "})\n",
    "# For Mobility data:\n",
    "mobility_clean = mobility_clean.rename(columns={\n",
    "    'country_region': 'country',\n",
    "    'sub_region_1': 'region',\n",
    "    'sub_region_2': 'subregion',\n",
    "})\n",
    "# Create a standard location ID for joining\n",
    "covid_long['location_id'] = covid_long['country'] + '_' + covid_long['region'].astype(str)\n",
    "mobility_clean['location_id'] = mobility_clean['country'] + '_' + mobility_clean['region'].fillna('Country-level').astype(str)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q6I_Sr7lxXi5"
   },
   "source": [
    "# Validate the cleaning was successful\n",
    "print(\"\\nStandardized location columns in COVID-19 data:\")\n",
    "print(covid_long[['country', 'region', 'location_id']].head())\n",
    "\n",
    "print(\"\\nStandardized location columns in Mobility data:\")\n",
    "print(mobility_clean[['country', 'region', 'location_id']].head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Justification: *I standardized the column names of the locations in both datasets and added a shared 'location_id' field that can be used for joining. This approach resolves the tidiness issue by establishing a uniform naming convention and joining process between datasets. The validation shows that both datasets now have consistent location identifiers, making it easy to integrate data in the future.*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remove unnecessary variables and combine datasets**\n",
    "\n",
    "Depending on the datasets, you can also peform the combination before the cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Remove unnecessary variables and combine datasets\n",
    "# Select relevant columns from each dataset\n",
    "covid_final = covid_long[['location_id', 'country', 'region', 'date', 'confirmed_cases']].copy()\n",
    "\n",
    "# Select relevant columns from mobility data\n",
    "mobility_cols = [\n",
    "    'location_id', 'country', 'region', 'date',\n",
    "    'retail_and_recreation_percent_change_from_baseline',\n",
    "    'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "    'parks_percent_change_from_baseline',\n",
    "    'transit_stations_percent_change_from_baseline',\n",
    "    'workplaces_percent_change_from_baseline',\n",
    "    'residential_percent_change_from_baseline'\n",
    "]\n",
    "mobility_final = mobility_clean[mobility_cols].copy()\n",
    "\n",
    "# Rename mobility columns to be more concise\n",
    "mobility_final = mobility_final.rename(columns={\n",
    "    'retail_and_recreation_percent_change_from_baseline': 'retail_change',\n",
    "    'grocery_and_pharmacy_percent_change_from_baseline': 'grocery_change',\n",
    "    'parks_percent_change_from_baseline': 'parks_change',\n",
    "    'transit_stations_percent_change_from_baseline': 'transit_change',\n",
    "    'workplaces_percent_change_from_baseline': 'workplace_change',\n",
    "    'residential_percent_change_from_baseline': 'residential_change'\n",
    "})\n",
    "\n",
    "# Combine the datasets on location_id and date\n",
    "combined_data = pd.merge(\n",
    "    covid_final,\n",
    "    mobility_final,\n",
    "    on=['location_id', 'date'],\n",
    "    suffixes=('_covid', '_mobility'),\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Handle duplicate columns from the merge\n",
    "combined_data = combined_data.rename(columns={\n",
    "    'country_covid': 'country',\n",
    "    'region_covid': 'region'\n",
    "})\n",
    "\n",
    "# Drop redundant columns\n",
    "combined_data = combined_data.drop(['country_mobility', 'region_mobility'], axis=1)\n",
    "\n",
    "# Display the combined dataset\n",
    "print(\"\\nCombined Dataset Preview:\")\n",
    "print(combined_data.head())\n",
    "print(f\"Combined Dataset Shape: {combined_data.shape}\")\n",
    "print(f\"Column names: {combined_data.columns.tolist()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F42urHuzttjF"
   },
   "source": [
    "## 4. Update your data store\n",
    "Update your local database/data store with the cleaned data, following best practices for storing your cleaned data:\n",
    "\n",
    "- Must maintain different instances / versions of data (raw and cleaned data)\n",
    "- Must name the dataset files informatively\n",
    "- Ensure both the raw and cleaned data is saved to your database/data store"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V3uay7EJUV_L"
   },
   "source": [
    "# Save clean versions of the data\n",
    "\n",
    "# Save the cleaned individual datasets\n",
    "covid_long.to_csv('data/covid19_clean.csv', index=False)\n",
    "mobility_final.to_csv('data/mobility_clean.csv', index=False)\n",
    "\n",
    "# Save the combined dataset\n",
    "combined_data.to_csv('data/covid_mobility_combined.csv', index=False)\n",
    "\n",
    "print(\"\\nAll datasets successfully saved to the data directory.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGy_yddGtzhM"
   },
   "source": [
    "## 5. Answer the research question\n",
    "\n",
    "### **5.1:** Define and answer the research question \n",
    "Going back to the problem statement in step 1, use the cleaned data to answer the question you raised. Produce **at least** two visualizations using the cleaned data and explain how they help you answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjedE4s4ZkEd"
   },
   "source": [
    "*Research question:* FILL IN from answer to Step 1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Lkw3rW9kZmOm"
   },
   "source": [
    "#Visual 1 - FILL IN"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer to research question:* FILL IN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6fdK_8ZGZm9R"
   },
   "source": [
    "#Visual 2 - FILL IN"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5RgvMGUZoHn"
   },
   "source": [
    "*Answer to research question:* FILL IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ezWXXZVj-TP"
   },
   "source": [
    "### **5.2:** Reflection\n",
    "In 2-4 sentences, if you had more time to complete the project, what actions would you take? For example, which data quality and structural issues would you look into further, and what research questions would you further explore?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XB3RBDG5kFe1"
   },
   "source": [
    "*Answer:* FILL IN"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
